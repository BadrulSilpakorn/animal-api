from flask import Flask, request, jsonify
import numpy as np
from PIL import Image
import io
import base64
import requests
import os
from datetime import datetime, timedelta

app = Flask(__name__)

# üîß Multi-TensorFlow Loader ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Render
def load_tflite():
    """‡∏•‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î TFLite ‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ"""
    try:
        # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: tflite-runtime
        import tflite_runtime.interpreter as tflite
        return tflite, "tflite-runtime"
    except ImportError:
        try:
            # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: tensorflow
            import tensorflow as tf
            return tf.lite, "tensorflow"
        except ImportError:
            return None, "none"

tflite_module, tf_type = load_tflite()
print(f"üß† Using: {tf_type}")

# ü§ñ Smart Model Loader
class SmartModelLoader:
    def __init__(self):
        self.interpreter = None
        self.input_details = None
        self.output_details = None
        self.model_type = None
        self.model_file = None
        self.loaded = False
        
    def try_load_model(self, model_path):
        """‡∏•‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß"""
        try:
            print(f"üîÑ Trying: {model_path}")
            
            if not os.path.exists(model_path):
                print(f"üìÑ Not found: {model_path}")
                return False
                
            if tf_type == "tflite-runtime":
                self.interpreter = tflite_module.Interpreter(model_path=model_path)
            else:
                self.interpreter = tflite_module.Interpreter(model_path=model_path)
            
            self.interpreter.allocate_tensors()
            self.input_details = self.interpreter.get_input_details()
            self.output_details = self.interpreter.get_output_details()
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö model type
            input_dtype = self.input_details[0]['dtype']
            self.model_type = "int8" if input_dtype == np.uint8 else "float32"
            self.model_file = model_path
            self.loaded = True
            
            print(f"‚úÖ Loaded: {model_path} ({self.model_type})")
            print(f"üìê Input: {self.input_details[0]['shape']}")
            return True
            
        except Exception as e:
            print(f"‚ùå Failed {model_path}: {e}")
            return False
    
    def load_any_model(self):
        """‡∏•‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå"""
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ (INT8 ‡∏Å‡πà‡∏≠‡∏ô)
        model_candidates = [
            "animal_model_int8_v1.tflite",      # Compatible INT8
            "animal_model_int8.tflite",         # Original INT8  
            "animal_model_float32_v1.tflite",   # Compatible Float32
            "animal_model_float32.tflite",      # Original Float32
            "model.tflite",                     # Generic
            "animal_classifier.tflite"          # Alternative
        ]
        
        for model_path in model_candidates:
            if self.try_load_model(model_path):
                return True
        
        print("‚ùå No compatible model found!")
        return False
    
    def predict(self, image_array):
        """‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢"""
        if not self.loaded:
            raise Exception("No model loaded")
            
        self.interpreter.set_tensor(self.input_details[0]['index'], image_array)
        self.interpreter.invoke()
        return self.interpreter.get_tensor(self.output_details[0]['index'])[0]

# ‡∏™‡∏£‡πâ‡∏≤‡∏á model loader
model = SmartModelLoader()

# Telegram config
TOKEN = os.getenv("TELEGRAM_TOKEN")
CHAT_ID = os.getenv("TELEGRAM_CHAT_ID")

def get_thai_time():
    return (datetime.utcnow() + timedelta(hours=7)).strftime('%H:%M:%S')

def send_message(text):
    if not TOKEN or not CHAT_ID:
        return False
    try:
        url = f"https://api.telegram.org/bot{TOKEN}/sendMessage"
        data = {"chat_id": CHAT_ID, "text": text, "parse_mode": "HTML"}
        response = requests.post(url, data=data, timeout=10)
        return response.status_code == 200
    except Exception as e:
        print(f"‚ùå Telegram error: {e}")
        return False

def send_photo(image_bytes, caption=""):
    if not TOKEN or not CHAT_ID:
        return False
    try:
        url = f"https://api.telegram.org/bot{TOKEN}/sendPhoto"
        files = {'photo': ('img.jpg', image_bytes, 'image/jpeg')}
        data = {'chat_id': CHAT_ID, 'caption': caption, 'parse_mode': 'HTML'}
        response = requests.post(url, files=files, data=data, timeout=15)
        return response.status_code == 200
    except Exception as e:
        print(f"‚ùå Photo error: {e}")
        return False

@app.route("/")
def home():
    # ‡∏•‡∏¥‡∏™‡∏ï‡πå‡πÑ‡∏ü‡∏•‡πå .tflite ‡∏ó‡∏µ‡πà‡∏°‡∏µ
    tflite_files = [f for f in os.listdir('.') if f.endswith('.tflite')]
    
    return jsonify({
        "status": "running",
        "tensorflow": tf_type,
        "model_loaded": model.loaded,
        "model_file": model.model_file if model.loaded else None,
        "model_type": model.model_type if model.loaded else None,
        "available_models": tflite_files,
        "time": get_thai_time(),
        "telegram_ready": bool(TOKEN and CHAT_ID)
    })

@app.route("/load-model")
def load_model():
    """‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà"""
    success = model.load_any_model()
    return jsonify({
        "success": success,
        "model_file": model.model_file if success else None,
        "model_type": model.model_type if success else None,
        "tensorflow": tf_type
    })

@app.route("/test")
def test():
    thai_time = get_thai_time()
    test_msg = f"ü§ñ <b>Model Test</b>\n"
    test_msg += f"üß† TF: {tf_type}\n"
    test_msg += f"ü§ñ Model: {model.model_type or 'None'}\n"
    test_msg += f"üìÑ File: {model.model_file or 'None'}\n"
    test_msg += f"‚è∞ Time: {thai_time}"
    
    result = send_message(test_msg)
    return jsonify({"sent": result, "time": thai_time})

@app.route("/predict", methods=["POST"])
def predict():
    try:
        # Auto-load model ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÇ‡∏´‡∏•‡∏î
        if not model.loaded:
            print("üîÑ Auto-loading model...")
            if not model.load_any_model():
                return jsonify({"error": "No compatible model available"}), 500
                
        if not request.json or "image" not in request.json:
            return jsonify({"error": "No image provided"}), 400

        # Decode image
        img_base64 = request.json["image"]
        original_bytes = base64.b64decode(img_base64)
        original_img = Image.open(io.BytesIO(original_bytes)).convert("RGB")
        
        print(f"üì∏ Original: {original_img.size}")
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏†‡∏≤‡∏û‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•
        target_size = model.input_details[0]['shape'][1:3]
        model_img = original_img.resize((target_size[1], target_size[0]))
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° input ‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÇ‡∏°‡πÄ‡∏î‡∏•
        if model.model_type == "float32":
            img_array = np.array(model_img, dtype=np.float32) / 255.0
        else:  # int8
            img_array = np.array(model_img, dtype=np.uint8)
            
        img_array = np.expand_dims(img_array, axis=0)
        
        print(f"ü§ñ Input: {img_array.shape} {img_array.dtype}")
        
        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
        output = model.predict(img_array)
        
        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
        labels = ["safe", "cow", "goat", "sheep"]
        
        if model.model_type == "float32":
            # Float32 output
            exp_out = np.exp(output - np.max(output))
            probs = exp_out / np.sum(exp_out)
        else:
            # INT8 output
            if output.dtype == np.uint8:
                probs = output.astype(np.float32) / 255.0
            else:
                exp_out = np.exp(output - np.max(output))
                probs = exp_out / np.sum(exp_out)
            
        pred_idx = np.argmax(probs)
        pred_label = labels[pred_idx]
        confidence = float(probs[pred_idx] * 100)
        
        print(f"üéØ {pred_label}: {confidence:.1f}%")
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏†‡∏≤‡∏û‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Telegram
        img_buffer = io.BytesIO()
        if max(original_img.size) > 1280:
            original_img.thumbnail((1280, 1280), Image.Resampling.LANCZOS)
        original_img.save(img_buffer, format='JPEG', quality=85, optimize=True)
        clean_img = img_buffer.getvalue()
        
        # ‡∏™‡πà‡∏á Telegram
        thai_time = get_thai_time()
        ALERT_THRESHOLD = 70.0
        
        if pred_label != "safe" and confidence > ALERT_THRESHOLD:
            # Alert
            caption = f"üö® <b>ALERT!</b>\nüêÑ {pred_label.upper()}: {confidence:.1f}%"
            send_photo(clean_img, caption)
            
            alert_msg = f"üö® <b>Animal Detected!</b>\n"
            alert_msg += f"üêÑ Type: <b>{pred_label.upper()}</b>\n"
            alert_msg += f"üìä Confidence: <b>{confidence:.1f}%</b>\n"
            alert_msg += f"‚è∞ Time: <b>{thai_time}</b>\n"
            alert_msg += f"ü§ñ Model: {model.model_type}"
            
            send_message(alert_msg)
        else:
            # Safe
            caption = f"‚úÖ <b>Area Clear</b>\nSafe: {confidence:.1f}%"
            send_photo(clean_img, caption)
            
            safe_msg = f"‚úÖ <b>All Clear</b>\n"
            safe_msg += f"üìä Confidence: <b>{confidence:.1f}%</b>\n"
            safe_msg += f"‚è∞ Time: <b>{thai_time}</b>"
            
            send_message(safe_msg)
        
        return jsonify({
            "status": "success",
            "prediction": pred_label,
            "confidence": round(confidence, 1),
            "all_predictions": {
                labels[i]: round(float(probs[i] * 100), 1) 
                for i in range(len(labels))
            },
            "alert": pred_label != "safe" and confidence > ALERT_THRESHOLD,
            "time": thai_time,
            "model_info": {
                "file": model.model_file,
                "type": model.model_type,
                "tensorflow": tf_type
            }
        })
        
    except Exception as e:
        error_msg = str(e)
        print(f"‚ùå Error: {error_msg}")
        
        try:
            error_alert = f"‚ùå <b>ERROR</b>\n{error_msg}\n‚è∞ {get_thai_time()}"
            send_message(error_alert)
        except:
            pass
        
        return jsonify({"status": "error", "error": error_msg}), 500

# Auto-load ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
print("üöÄ Starting Smart Animal Detection Server...")
print(f"üß† TensorFlow: {tf_type}")
model.load_any_model()

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 5000))
    app.run(host="0.0.0.0", port=port, debug=False)
